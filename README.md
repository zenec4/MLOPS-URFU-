# mlops_pracs 
Практические работы по предмету "Автоматизация машинного обучения"
## Участники команды
РИМ-130906 Томилов Максим, Шишкин Михаил, Нуртдинов Дмитрий, Степанова Элина 
### Module 1
<details>

- Необходимо из создать простейший конвейер для автоматизации работы с моделью машинного обучения.
- Отдельные этапы конвейера машинного обучения описываются в разных python–скриптах, которые потом соединяются в единую цепочку действий с помощью bash-скрипта.
- Все файлы необходимо разместить в подкаталоге lab1 корневого каталога
  
Этапы:

1. Создаем python-скрипт (data_creation.py), который создает различные наборы данных, описывающие некий процесс (например, изменение дневной температуры). Таких наборов несколько, в некоторые данные можно включить аномалии или шумы. Часть наборов данных должны быть сохранены в папке “train”, другая часть в папке “test”. Одним из вариантов выполнения этого этапа может быть скачивание набора данных из сети, и разделение выборки на тестовую и обучающую. Учтите, что файл должен быть доступен и методы скачивания либо есть в ubuntu либо устанавливаются через pip в файле pipeline.sh
   
2. Создаем python-скрипт (data_preprocessing.py), который выполняет предобработку данных, например, с помощью sklearn.preprocessing.StandardScaler. Трансформации выполняются и над тестовой и над обучающей выборкой.
   
3. Создаем python-скрипт (model_preparation.py), который создает и обучает модель машинного обучения на построенных данных из папки “train”. Для сохранения модели в файл можно воспользоваться [pickle](https://docs.python.org/3/library/pickle.html) (см. пример)

4. Создаем python-скрипт (model_testing.py), проверяющий модель машинного обучения на построенных данных из папки “test”.

5. Пишем bash-скрипт (pipeline.sh), последовательно запускающий все python-скрипты. При необходимости усложните скрипт. В результате выполнения скрипта на терминал в стандартный поток вывода печатается одна строка с оценкой метрики на вашей модели, например:


#### Как запустить?

<details>

#### Использование
Для того чтобы запустить данный pipeline необходимо:
1. Клонировать данный репозиторий на ПК или ВМ (с ОС Linux)
2. Сделать файл **pipeline.sh** исполняемым, выполнив в терминале в каталоге с файлами проекта команду
```
chmod +x pipeline.sh
```
3. Запустить bash-скрипт **pipeline.sh**, выполнив в терминале в каталоге с файлами проекта любую из представленных команд:
```
./pipeline.sh
```

</details>
</details>

### Module 2
<details>

* Вам нужно разработать собственный конвейер автоматизации для проекта машинного обучения. Для этого вам понадобится виртуальная машина с установленным Jenkins, python и необходимыми библиотеками. В ходе выполнения практического задания вам необходимо автоматизировать сбор данных, подготовку датасета, обучение модели и работу модели.  

Этапы задания 
1. Развернуть сервер с Jenkins, установить необходимое программное обеспечение для работы над созданием модели машинного обучения.
2. Выбрать способ получения данных (скачать из github, из Интернета, wget, SQL запрос, …). 
3. Провести обработку данных, выделить важные признаки, сформировать датасеты для тренировки и тестирования модели, сохранить.
4. Создать и обучить на тренировочном датасете модель машинного обучения, сохранить в pickle или аналогичном формате. !
5. Загрузить сохраненную модель на ​предыдущем этапе и проанализировать ее качество на тестовых данных.

Отчёт по данному заданию находится в папке lab2

</details>

### Module 3
<details>
В практическом задание по модулю вам необходимо применить полученные знания по работе с docker (и docker-compose). Вам необходимо использовать полученные ранее знания по созданию микросервисов. В этом задании необходимо развернуть микросервис в контейнере докер. Например, это может быть модель машинного обучения, принимающая запрос по API и возвращающая ответ. Вариантом может быть реализация приложения на основе streamlit (https://github.com/korelin/streamlit_demo_app). Результаты работы над этой работой стоит поместить в подкаталог lab3 вашего корневого каталога репозитория. Что необходимо выполнить:

1. Подготовить python код для модели и микросервиса
2. Создать Docker file
3. Создать docker образ
4. Запустить docker контейнер и проверить его работу
5. Дополнительными плюсами будут:

Использование docker-compose
Автоматизация сборки образа привязка имени тэга к версии сборки (sha-коммита, имя ветки)
Деплой (загрузка) образа в хранилище артефактов например dockerhub

### Использование
<details>
  
### Для запуска Docker в терминал пишем следующие команды:
  
docker build -t app:latest -f Dockerfile .
  
docker run app:latest -p 8501:8501
     
Первая команда выполняет создание Docker-образа.

Вторая команда запускает Docker.

### Для запуска Docker-compose в терминал пишем следующюю команду:

docker-compose up

</details>
</details>

### Module 4
<details>
В практическом задании данного модуля вам необходимо продемонстрировать навыки практического использования утилиты dvc для работы с данными. В результате выполнения этих заданий вы выполните все основные операции с dvc и закрепите полученные теоретические знания практическими действиями.

Этапы задания:

Создайте папку lab4 в корне проекта.
1. Установите git и dvc. Настройте папку проекта для работы с git и dvc.
 
2. Настройте удаленное хранилище файлов, например на Google Disk или S3.
   
4. Создайте датасет, например, о пассажирах “Титаника” catboost.titanic().
   
6. Модифицируйте датасет, в котором содержится информация о классе (“Pclass”), поле (“Sex”) и возрасте (“Age”) пассажира. Сделайте коммит в git и push в dvc.
   
8. Создайте новую версию датасета, в котором пропущенные (nan) значения в поле “Age” будут заполнены средним значением. Сделайте коммит в git и push в dvc.
   
10. Создайте новый признак с использованием one-hot-encoding для строкового признака “Пол” (“Sex”). Сделайте коммит в git и push в dvc.
    
11. Выполните переключение между всеми созданными версиями датасета.
    
При правильном выполнении задания и вас появится git репозиторий с опубликованной метаинформацией и папка на Google Disk, в которой хранятся различные версии датасетов. Вам необходимо подготовить отчет в тех функциональностях которые вы настроили. Дополнительно можно настроить DAG, запуск и версионирование экспериментов, например, с использованием Hydra.

В постановке задачи используется датасет из конкурса “Titanic Disaster”, однако вы можете использовать свои наборы данных, в этом случае в п.п.4-8 необходимо использовать информацию и признаки из вашего датасета.
### Использование
<details>
  Команды которые вводились в Terminal:

  
  pip install dvc
  
  В репозитории пишем команду git init и dvc init
  
  mkdir lab4

  git commit -am "init dvc"

  ### Далее, был создан датасет с помощью скрипта "dataset1.py", он находится в папке lab4 в данном репозитории

  dvc add lab4/

  git add .gitignore lab4.dvc

  dvc config core.autostage true

  git commit -m "add new titanic.csv"

  ### Далее было настроено удалённое хранилище Google Диск: https://drive.google.com/drive/folders/1e1gM-Jvv_zDv2l5lx9vhJCuwUS_-jduB

  dvc remote add --default myremote gdrive://1e1gM-Jvv_zDv2l5lx9vhJCuwUS_-jduB

  dvc remote modify myremote gdrive_acknowledge_abuse true  

  git add .dvc/config

  git commit -m "Google drive storage added for remote"

  dvc push - после выполнения этой команды, была произведена авторизация на Google Диске

  dvc push -r myremote

  ### Далее, был изменён датасет с помощью скрипта "dataset2.py", он находится в папке lab4 в данном репозитории (В данном скрипте был сразу модифицирован датасет и пропущенные значения nan были заменены на среднее значение, т.е. задание 7 и 8 было объединено)

  dvc add lab4

  dvc push -r myremote
  ### Команды которые были использованы для перемещения между созданными версиями датасета:

  rm -rf lab4

  dvc pull -r myremote
</details>
